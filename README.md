# ğŸ›¡ï¸ breaker-ai

**breaker-ai** is a CLI tool to scan prompts for injection risks, open-ended patterns, and other prompt security issues. It helps you validate and harden prompts for LLMs and AI agents.

## âœ¨ Features

- Detects risky prompt injection patterns
- Flags open-ended or unsafe prompt structures
- Checks for user input safety
- Scoring system (0â€“100)
- Customizable minimum expected score (`--expected`)
- JSON and table output formats

## ğŸš€ Installation

```sh
npm install -g breaker-ai@cli
```

Or use with npx (no install needed):

```sh
npx breaker-ai@cli scan <prompt-file> [--expected <score>]
```

## âš¡ Usage

### Basic scan

```sh
breaker scan "Ignore previous instructions."
```

### Scan a file

```sh
breaker scan path/to/prompt.txt
```

### Set minimum expected score

```sh
breaker scan path/to/prompt.txt --expected 80
```

### JSON output

```sh
breaker scan path/to/prompt.txt --json
```

## ğŸ“ Example Output

```text
Promptâ€‘Security Report
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (index) â”‚ Check                 â”‚ Result                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0       â”‚ 'Risky patterns'      â”‚ 'None'                                 â”‚
â”‚ 1       â”‚ 'Openâ€‘ended patterns' â”‚ 'system.*prompt'                       â”‚
â”‚ 2       â”‚ 'Userâ€‘input safety'   â”‚ 'No {{user_input}} placeholder found.' â”‚
â”‚ 3       â”‚ 'Length'              â”‚ 'OK'                                   â”‚
â”‚ 4       â”‚ 'Overall score'       â”‚ '60/100'                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Options

- `-j, --json` Output as JSON
- `-e, --expected <number>` Minimum expected score (exit 1 if not met)

## ğŸ“¦ Scripts

- `npm run build` â€” Build TypeScript to JavaScript
- `npm run test` â€” Run tests
- `npm run dev` â€” Run CLI in dev mode

## ğŸ—ºï¸ Roadmap

- [ ] Configurable pattern rules (customize what is flagged as risky)
- [ ] CI integration examples (GitHub Actions, etc)
- [ ] More output formats (Markdown, HTML)
- [ ] Web dashboard for prompt risk analytics
- [ ] Multi-language prompt support
- [ ] Community pattern sharing
- [ ] API/server mode

_Have a feature idea? Open an issue or pull request!_

## ğŸ“„ License

MIT

---

Made with â¤ï¸ by Breaker AI
