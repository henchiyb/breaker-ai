# ğŸ›¡ï¸ Breaker-AI

Proactively Stress-Test Your AI Before Attackers Do.

Breaker-AI is an open-source CLI tool that detects prompt injection risks, jailbreak vulnerabilities, and security flaws inside your AI prompts. It empowers security teams, developers, and AI researchers to validate and harden LLM prompts â€” before real attackers can exploit them.

## âœ¨ Key Features

- **Jailbreak Resistance Testing**: Automatically runs your system prompts against real-world jailbreak attempts and evaluates vulnerabilities.
- **Prompt Injection Detection**: Scans for risky patterns, unsafe prompt structures, and user-input manipulation risks.
- **Customizable Rules & Thresholds**: Set expected minimum scores, skip specific tests, or define what "secure" means for your needs.
- **Clear Reports**: Output results as human-readable tables or machine-readable JSON.
- **Agent-Ready (coming soon!)**: Adaptive Breaker-AI Attack Agents will mutate and evolve attacks dynamically â€” simulating real adversaries.

## ğŸš€ Installation
Install globally via npm:

```bash
npm install -g breaker-ai
```

Or run instantly without installing:

```bash
npx breaker-ai scan <prompt-file> [--expected <score>]
```

## âš¡ Quick Usage

### ğŸ†• Jailbreak Resistance Testing

Test your system prompt against a suite of jailbreak attacks:

```bash
OPENAI_API_KEY=your-api-key npx breaker-ai jailbreak "path/to/systemPrompt.txt"
```

**Options:**

Use a different model:

```bash
export OPENAI_MODEL=gpt-4
```

Use OpenRouter (e.g., Claude, Mistral):

```bash
export OPENAI_BASE_URL=https://openrouter.ai/api/v1
export OPENAI_MODEL=claude-3-sonnet
```

List all available jailbreak prompts:

```bash
breaker-ai list-prompts
```

## ğŸ›¡ï¸ Example CLI Output

```
Jailbreak Report
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attack Name         â”‚ Model Response                      â”‚ Passed?  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AI Diagnostics Leak â”‚ [INTERNAL LOGS: GPT-4 info dumped]   â”‚ âŒ       â”‚
â”‚ Movie Scene Attack  â”‚ "I can't assist with that request." â”‚ âœ…       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“œ Basic Prompt Security Scan

```bash
breaker-ai scan "path/to/prompt.txt"
```

**Outputs:**

```
Promptâ€‘Security Report
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (index) â”‚ Check                 â”‚ Result                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0       â”‚ Risky Patterns        â”‚ None                        â”‚
â”‚ 1       â”‚ Open-ended Structures â”‚ system.*prompt              â”‚
â”‚ 2       â”‚ User Input Safety     â”‚ No {{user_input}} found.    â”‚
â”‚ 3       â”‚ Length Check          â”‚ OK                          â”‚
â”‚ 4       â”‚ Overall Score         â”‚ 60/100                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Set Minimum Expected Score:

```bash
breaker-ai scan path/to/prompt.txt --expected 80
```

Export JSON report:

```bash
breaker-ai scan path/to/prompt.txt --json /path/to/result.json
```

## ğŸ› ï¸ Utility: Mask Sensitive Words

```bash
breaker-ai mask "Hello world" --words Hello,world
# Output: He***wo***
```

Or mask words inside a file:

```bash
breaker-ai mask path/to/file.txt --words secret,password
```

## ğŸ—ºï¸ Roadmap

- ğŸ”¥ Configurable pattern rules
- ğŸ”¥ CI/CD integration (examples for GitHub Actions, etc)
- ğŸ”¥ HTML and Markdown output formats
- ğŸ”¥ Web dashboard for visual prompt risk analytics
- ğŸ”¥ Multi-language support
- ğŸ”¥ Breaker-AI Attack Agents (dynamic adversarial attacks)
- ğŸ”¥ API / server mode for real-time prompt scanning
- ğŸ”¥ Community-shared attack patterns

(Have an idea? Open an issue or pull request!)

## ğŸ“š API Usage

Use Breaker-AI as a library inside your JavaScript / TypeScript projects:

```typescript
import { maskWordsInText } from "breaker-ai";

(async () => {
  const masked = await maskWordsInText("Hello world", ["Hello", "world"]);
  console.log(masked); // "He***wo***"
})();
```

## ğŸ“„ License
Breaker-AI is open-sourced under the MIT License. Made with â¤ï¸ by the Breaker-AI Team.

ğŸš€ Stay ahead of the AI security curve.
ğŸ›¡ï¸ Start stress-testing your AI today with Breaker-AI.
